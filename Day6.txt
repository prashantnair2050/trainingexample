AWS Bigdata Training
Trainer: Prashant Nair
============================================================================================================================

Agenda
-------
Working with AWS Glue
Working with AWS Athena
Rev- Instantiating and working on PySpark Notebook


PySpark in Azure
	- Databricks	
	- SparkPool (Synapse Analytics)


Pyspark in AWS
	- EMR (Elastic MapReduce)
		Its a managed Hadoop cluster. 


Instantiating a PySpark Jupyter Lab
------------------------------------

1. Go to EMR Dashboard
2. Click on Notebooks > Click on Create Notebook
3. Give a name to the notebook
4. Select Create Cluster
4. Keep everything default and click on Create Notebook


Test the notebook and try some pyspark examples

df=spark.read.option("header",True).option("inferSchema",True).csv("s3://txnsbucket876/transactions/txns")


Assignment - USing Transactions Dataset

00000006,10-28-2011,4002190,027.89,Puzzles,Jigsaw Puzzles,Charleston,South Carolina,credit

Header is absent

txnid 
txndate  (Preferred to use string for date as formatting is not standard)
custid
amount
category
subcategory
city
state
txntype


Perform the following tasks
1. Find the total revenue generated based on category
2. Find the total number of transactions done by credit and cash
3. Find the total amount generated by credit 
4. Find the highest selling category
5. Find the lowest selling category



Working with Datawarehousing Solutions
========================================

Azure:
	- Azure Synapse Analytics (Serverless - Standalone)
				  (Server-based - SqlPool, SparkPool)					


AWS:
	- AWS Redshift (Server-based Arch)
	- AWS Athena	(Server-less Arch)




					Data warehousing Solutions
						|
			---------------------------------------------------------
			|							|
	Server-less Architecture					Server-based Architecture




AWS Athena
===========

- Serverless Interactive Query Services for S3
- Deal with:
	a. Delimited Files
	b. XML
	c. JSON
	d. Parquet
	e. ORC
	f. Avro

- Athena requires two components:
	a. Data in S3
	b. Metadata in AWS Glue




Using AWS Glue to create schema using Crawler
==============================================

Using Glue Crawler
===================
Automobile Dataset----> S3 ------> AWS Glue


1. Create S3 bucket and load data in it
2. Go to Glue Dashboard > Crawler > Add Crawler. Follow the instructions and connect with the data source at folder level and Run the Crawler.




Using Manual Method
======================
employee ----> S3 ------>AWS Glue ------> Athena


1. Create S3 bucket and load data in it
2. Create Metadata for automobile dataset in Glue
	- Go to Glue Dashboard
	- On left hand menu click Databases > Add database > Create database named           'tigeranalytics'
	- Click on Tables > Add Table > Add Table Manually 
		- Give table name as 'employee' and select database 'tigeranalytics' > Next
		- Select Data Store > S3 > Set the Location of dataset > Next
		- Choose Data Format > CSV > Delimiter as , > Next
		- Create Schema with appropriate datatypes > Next
		- Skip Partition Indices as we didnt set any partition key > Next
		- Review the settings > Finish
		

3. Create Athena Instance and connect with metadata for Interactive Query
	- Go to Athena Dashboard and Click on Get Started

 Athena will automatically interact with Glue for Catalog and help provide platform for Interactive Query !







AWS Glue + Athena Assignment
=============================


txnsSmall

Perform both methods

Perform the following tasks
1. Find the total revenue generated based on category
2. Find the total number of transactions done by credit and cash
3. Find the total amount generated by credit 
4. Find the highest selling category
5. Find the lowest selling category
























































